{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "HERE = Path(__file__).parent.resolve()\n",
    "DATA_DIR = HERE\n",
    "\n",
    "OUT_DIR = HERE / \"clean\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Interpolate only small gaps (hours)\n",
    "MAX_GAP_HOURS = 3\n",
    "\n",
    "# After interpolation, fill longer gaps but only up to this many hours\n",
    "FILL_LIMIT_HOURS = 24\n",
    "\n",
    "# Physical plausibility bounds (adjust if units differ)\n",
    "BOUNDS = {\n",
    "    \"PM25\":  (0, 1000),\n",
    "    \"PM10\":  (0, 1500),\n",
    "    \"NO2\":   (0, 1000),\n",
    "    \"SO2\":   (0, 1000),\n",
    "    \"O3\":    (0, 1000),\n",
    "    \"CO\":    (0, 50000),\n",
    "    \"NOX\":   (0, 2000),\n",
    "    \"WS\":    (0, 60),\n",
    "    \"WD\":    (0, 360),\n",
    "    \"AT\":    (-5, 50),\n",
    "    \"RH\":    (0, 100),\n",
    "    \"BP\":    (800, 1100),\n",
    "    \"SolarRad\": (0, 1400),\n",
    "    \"Rain\":  (0, 500),\n",
    "}\n",
    "\n",
    "STATION_META = {\n",
    "    \"Battaramulla\": {\"lat\": 6.901035, \"lon\": 79.926513},\n",
    "    \"Kandy\":        {\"lat\": 7.292651, \"lon\": 80.635649},\n",
    "}\n",
    "\n",
    "# Final columns used by the new DL model\n",
    "SCHEMA = [\n",
    "    \"station\", \"lat\", \"lon\", \"datetime\",\n",
    "    \"PM25\",\"PM10\",\"NO2\",\"SO2\",\"O3\",\"CO\",\"NOX\",\n",
    "    \"WS\",\"WD\",\"AT\",\"RH\",\"BP\",\"SolarRad\",\"Rain\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "\n",
    "COLMAP = {\n",
    "    # datetime\n",
    "    \"period start time\": \"datetime\",\n",
    "    \"period start\": \"datetime\",\n",
    "    \"date time\": \"datetime\",\n",
    "    \"timestamp\": \"datetime\",\n",
    "    \"datetime\": \"datetime\",\n",
    "    \"date\": \"datetime\",\n",
    "\n",
    "    # PM / gases\n",
    "    \"pm2.5 conc\": \"PM25\", \"pm2.5\": \"PM25\", \"pm 2.5\": \"PM25\", \"pm₂.₅\": \"PM25\", \"pm2.5 concentration\": \"PM25\",\n",
    "    \"pm10 conc\": \"PM10\",  \"pm10\": \"PM10\",  \"pm 10\": \"PM10\",  \"pm10 concentration\": \"PM10\",\n",
    "    \"no2 conc\": \"NO2\",    \"no2\": \"NO2\",    \"no2 concentration\": \"NO2\",\n",
    "    \"so2 conc\": \"SO2\",    \"so2\": \"SO2\",    \"so2 concentration\": \"SO2\",\n",
    "    \"o3 conc\": \"O3\",      \"o3\": \"O3\",      \"o3 concentration\": \"O3\",\n",
    "    \"co conc\": \"CO\",      \"co\": \"CO\",      \"co concentration\": \"CO\",\n",
    "    \"nox conc\": \"NOX\",    \"nox\": \"NOX\",    \"nox concentration\": \"NOX\",\n",
    "\n",
    "    # met\n",
    "    \"ws average\": \"WS\", \"ws\": \"WS\", \"wind speed\": \"WS\", \"windspeed\": \"WS\",\n",
    "    \"wd average\": \"WD\", \"wd\": \"WD\", \"wind direction\": \"WD\", \"winddirection\": \"WD\",\n",
    "    \"at\": \"AT\", \"ambient temperature\": \"AT\", \"temperature\": \"AT\",\n",
    "    \"rh\": \"RH\", \"relative humidity\": \"RH\", \"humidity\": \"RH\",\n",
    "    \"bp\": \"BP\", \"barometric pressure\": \"BP\", \"pressure\": \"BP\",\n",
    "    \"solar rad\": \"SolarRad\", \"solar radiation\": \"SolarRad\", \"solar\": \"SolarRad\",\n",
    "    \"rain gauge\": \"Rain\", \"rain\": \"Rain\", \"rainfall\": \"Rain\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    m = {}\n",
    "    for c in df.columns:\n",
    "        nc = norm(c)\n",
    "        if nc in COLMAP:\n",
    "            m[c] = COLMAP[nc]\n",
    "    return df.rename(columns=m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_data_files(base: Path) -> list[Path]:\n",
    "    return [p for p in base.rglob(\"*\") if p.suffix.lower() in (\".xlsx\", \".xls\", \".csv\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_from_filename(path: Path) -> str | None:\n",
    "    n = path.name.lower()\n",
    "    if \"battaramulla\" in n:\n",
    "        return \"Battaramulla\"\n",
    "    if \"kandy\" in n:\n",
    "        return \"Kandy\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        if path.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "            return pd.read_excel(path, engine=\"openpyxl\")\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {path.name}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_bounds(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    low, high = BOUNDS[col]\n",
    "    bad = (df[col] < low) | (df[col] > high)\n",
    "    if bad.any():\n",
    "        df.loc[bad, col] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_hourly(path: Path) -> pd.DataFrame:\n",
    "    raw = read_any(path)\n",
    "    if raw.empty:\n",
    "        return raw\n",
    "\n",
    "    df = rename_columns(raw).copy()\n",
    "\n",
    "    stn = station_from_filename(path)\n",
    "    if stn is None:\n",
    "        print(f\"[WARN] Could not infer station from filename: {path.name} (skipping)\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df[\"station\"] = stn\n",
    "    df[\"lat\"] = STATION_META[stn][\"lat\"]\n",
    "    df[\"lon\"] = STATION_META[stn][\"lon\"]\n",
    "\n",
    "    if \"datetime\" not in df.columns:\n",
    "        print(f\"[WARN] No datetime column in {path.name} (skipping)\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"datetime\"]).copy()\n",
    "\n",
    "    # Hour alignment (indexing only; we won't create hour features)\n",
    "    df[\"datetime\"] = df[\"datetime\"].dt.floor(\"h\")\n",
    "\n",
    "    for c in SCHEMA:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    out = df[SCHEMA].copy()\n",
    "\n",
    "    # numeric coercion\n",
    "    for c in SCHEMA:\n",
    "        if c in (\"station\", \"datetime\"):\n",
    "            continue\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_station_block(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Continuous hourly timeline\n",
    "    - Small-gap interpolation\n",
    "    - Limited ffill/bfill\n",
    "    - Final fallback station median\n",
    "    \"\"\"\n",
    "    g = g.sort_values(\"datetime\").copy()\n",
    "    g = g.drop_duplicates(subset=[\"datetime\"], keep=\"last\")\n",
    "    g = g.set_index(\"datetime\")\n",
    "\n",
    "    full_idx = pd.date_range(g.index.min(), g.index.max(), freq=\"h\")\n",
    "    g = g.reindex(full_idx)\n",
    "\n",
    "    g[\"station\"] = g[\"station\"].ffill().bfill()\n",
    "    g[\"lat\"] = g[\"lat\"].ffill().bfill()\n",
    "    g[\"lon\"] = g[\"lon\"].ffill().bfill()\n",
    "\n",
    "    numeric_cols = [\"PM25\",\"PM10\",\"NO2\",\"SO2\",\"O3\",\"CO\",\"NOX\",\"WS\",\"WD\",\"AT\",\"RH\",\"BP\",\"SolarRad\",\"Rain\"]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col not in g.columns:\n",
    "            g[col] = np.nan\n",
    "\n",
    "        # bounds clip before fill\n",
    "        if col in BOUNDS:\n",
    "            g = clip_bounds(g, col)\n",
    "\n",
    "        # small gap interpolation\n",
    "        g[col] = g[col].interpolate(\"time\", limit=MAX_GAP_HOURS, limit_direction=\"both\")\n",
    "\n",
    "        # limited ffill/bfill for longer gaps\n",
    "        g[col] = g[col].ffill(limit=FILL_LIMIT_HOURS).bfill(limit=FILL_LIMIT_HOURS)\n",
    "\n",
    "        # final fallback median\n",
    "        med = float(pd.to_numeric(g[col], errors=\"coerce\").median())\n",
    "        if not np.isfinite(med):\n",
    "            med = 0.0\n",
    "        g[col] = g[col].fillna(med)\n",
    "\n",
    "        # bounds clip after fill too\n",
    "        if col in BOUNDS:\n",
    "            g = clip_bounds(g, col)\n",
    "\n",
    "    g = g.reset_index().rename(columns={\"index\": \"datetime\"})\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_summary(df: pd.DataFrame, label: str):\n",
    "    if df.empty:\n",
    "        print(f\"[SUM] {label}: 0 rows\")\n",
    "        return\n",
    "    print(f\"[SUM] {label}: {len(df):,} rows\")\n",
    "    print(\"      stations:\", df[\"station\"].unique().tolist())\n",
    "    print(f\"      time range: {df['datetime'].min()} → {df['datetime'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    all_files = find_all_data_files(DATA_DIR)\n",
    "    if not all_files:\n",
    "        print(f\"ERROR: No .xlsx/.xls/.csv found in {DATA_DIR}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    hourly_files = [p for p in all_files if station_from_filename(p) is not None]\n",
    "\n",
    "    print(\"----- DISCOVERY -----\")\n",
    "    print(f\"Base folder: {DATA_DIR}\")\n",
    "    print(f\"Total files found: {len(all_files)}\")\n",
    "    print(f\"Hourly candidates (Battaramulla/Kandy): {len(hourly_files)}\")\n",
    "    for f in hourly_files:\n",
    "        print(\"  -\", f.name)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    if not hourly_files:\n",
    "        print(\"[ERROR] No Battaramulla/Kandy files detected by filename.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    parts = []\n",
    "    for f in hourly_files:\n",
    "        d = standardize_hourly(f)\n",
    "        if d.empty:\n",
    "            print(f\"[WARN] Skipped: {f.name}\")\n",
    "            continue\n",
    "        for col in BOUNDS:\n",
    "            if col in d.columns:\n",
    "                d = clip_bounds(d, col)\n",
    "        parts.append(d)\n",
    "        print(f\"[OK] Loaded: {f.name} rows={len(d):,}\")\n",
    "\n",
    "    if not parts:\n",
    "        print(\"[ERROR] No usable tables parsed.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    hourly = pd.concat(parts, ignore_index=True)\n",
    "    hourly = (\n",
    "        hourly.sort_values([\"station\", \"datetime\"])\n",
    "             .drop_duplicates([\"station\", \"datetime\"], keep=\"last\")\n",
    "             .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    hourly_clean = (\n",
    "        hourly.groupby(\"station\", group_keys=False)\n",
    "              .apply(fill_station_block)\n",
    "              .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Add ONLY month/day (no hour)\n",
    "    dt = pd.to_datetime(hourly_clean[\"datetime\"])\n",
    "    hourly_clean[\"month\"] = dt.dt.month.astype(int)\n",
    "    hourly_clean[\"day\"] = dt.dt.day.astype(int)\n",
    "\n",
    "    hourly_clean[\"hour\"] = dt.dt.hour.astype(int)\n",
    "    hourly_clean[\"hour_sin\"] = np.sin(2 * np.pi * hourly_clean[\"hour\"] / 24)\n",
    "    hourly_clean[\"hour_cos\"] = np.cos(2 * np.pi * hourly_clean[\"hour\"] / 24)\n",
    "\n",
    "    out_path = OUT_DIR / \"cea_hourly_2019_2024_clean_dl_md.csv\"\n",
    "    hourly_clean.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"\\n[OK] Saved DL-ready dataset (month+day only) → {out_path}\")\n",
    "    quick_summary(hourly_clean, \"hourly_clean_md\")\n",
    "\n",
    "    # Missing report (should be near zero after filling)\n",
    "    numeric_cols = [\"PM25\",\"PM10\",\"NO2\",\"SO2\",\"O3\",\"CO\",\"NOX\",\"WS\",\"WD\",\"AT\",\"RH\",\"BP\",\"SolarRad\",\"Rain\"]\n",
    "    miss = (hourly_clean[numeric_cols].isna().mean() * 100.0).sort_values(ascending=False)\n",
    "    print(\"\\n[INFO] Missing % by column (after cleaning):\")\n",
    "    print(miss.round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
